{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Catalog Cleanup for Informatica Migration\n",
    "\n",
    "First, we import the necessary libraries for this work.\n",
    "***Note***: a **.env** file is needed wherever this notebook is stored, which will have the Environment Variables:\n",
    "- CP4D_PROD_URL: The url for the PROD CP4D instance, and it must contain 'https://' prefixed.\n",
    "- CP4D_USERNAME: Your MSK ID name.\n",
    "- USERPASS: Your MSK ID Password."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pymssql\n",
    "import pyodbc\n",
    "import requests\n",
    "import xlsxwriter\n",
    "import ssl\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',150000)\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.width', 10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we do an API call to the CP4D instance to retrieve all the Concepts and their metadata, such as definition, approval data, authoritative sources.\n",
    "This will be saved in an Excel spreadsheet for documentation and further manipulation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# url = os.getenv(\"CP4D_PROD_URL\")\n",
    "# # url = os.getenv(\"CP4D_QA_URL\")\n",
    "# user = os.getenv(\"CP4D_USERNAME\")\n",
    "# pwd = os.getenv(\"USERPASS\")\n",
    "# pd.set_option('display.max_rows',150000)\n",
    "# pd.set_option('display.max_columns',10)\n",
    "# pd.set_option('display.width', 10000)\n",
    "#\n",
    "# # logging.basicConfig(filename='CPAPI.log',\n",
    "# #                     filemode='a',\n",
    "# #                     format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "# #                     datefmt='%H:%M:%S',\n",
    "# #                     level=logging.INFO)\n",
    "# body = {\n",
    "#   \"password\": pwd,\n",
    "#   \"username\": user,\n",
    "#   \"useLdapGroup\": False,\n",
    "# }\n",
    "# auth = requests.post(f\"{url}/icp4d-api/v1/authorize\", json=body)\n",
    "# # print(auth.text)\n",
    "# # print(json.dumps(auth, indent=3))\n",
    "# headers = {\"Authorization\": f\"Bearer {auth.json()['token']}\"}\n",
    "# paths={'projects':'/v2/projects',\n",
    "#        'projects_v1':'/ibm/iis/dq/da/rest/v1/workspaces',\n",
    "#        'catalogs':'/v2/catalogs',\n",
    "#        'assets': '/v2/assets',\n",
    "#        'asset_types':'/v2/asset_types',\n",
    "#        'asset_files':'/v2/asset_files',\n",
    "#        'data_assets': '/v2/asset_types/data_asset/search',\n",
    "#        'data_class': '/v3/data_classes',\n",
    "#        'dc_search': '/v3/data_classes/relationships/search',\n",
    "#        'folder_assets': '/v2/folder_assets',\n",
    "#        'terms':'/v3/glossary_terms',\n",
    "#        'export_terms':'/v3/governance_artifact_types/glossary_term/export',\n",
    "#        'rules':'/v3/rules/relationships/search',\n",
    "#        'reference':'/v3/reference_data',\n",
    "#        'cnxs':'/v2/connections',\n",
    "#        'jobs':'/v2/jobs'}\n",
    "#\n",
    "# btdl = requests.get(f\"{url}{paths['export_terms']}\", headers=headers).text\n",
    "# writer = pd.ExcelWriter('WKC-Extract-PROD.xlsx', engine='xlsxwriter')\n",
    "# btdl = pd.read_csv(StringIO(btdl),sep=',').to_excel(writer,index='False')\n",
    "# writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then load the Concepts for the comparison work via a Pandas dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "wkc = pd.read_excel('WKC-Extract-PROD.xlsx')\n",
    "wkc = wkc[['Name', 'Description','Stewards', 'Business Start', 'Data Classes','custom_00. Date Approved by Stewardship Council', 'custom_03. Authoritative Source System', 'custom_04. Authoritative Source Table', 'custom_05. Authoritative Source Column']]\n",
    "wkc.columns = ['Name', 'Description','Stewards', 'Business Start', 'Data Classes','Stewardship Council Approval Date', 'Authoritative Source System', 'Authoritative Source Table', 'Authoritative Source Column']\n",
    "\n",
    "\n",
    "# print(iic2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The stewardship working templates should be saved, preferably in a folder, where we can read through them and capture the necessary tabs that contain the concepts and all the stewardship work done for them for each of our domains.\n",
    "A cleanup depending on the domain will be done to make sure it aligns with the WKC information for comparison.\n",
    "The ***udf*** variable used below will be our *Master* list of concepts and definitions we will be changing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Tracking & Resources on file Stewardship-CIS.xlsm\n",
      "Processing Global Checklist on file Stewardship-CIS.xlsm\n",
      "Processing ClinDoc-MVP on file Stewardship-CIS.xlsm\n",
      "Processing Meds-OrderedMVPColumns on file Stewardship-CIS.xlsm\n",
      "Processing Meds-AdminColumns on file Stewardship-CIS.xlsm\n",
      "Processing Meds-OrderNonMVPColumns on file Stewardship-CIS.xlsm\n",
      "Processing Lists on file Stewardship-CIS.xlsm\n",
      "Processing Guidelines & Checklist on file Stewardship-Telemedicine.xlsx\n",
      "Processing Authoritative Source Tables on file Stewardship-Telemedicine.xlsx\n",
      "Processing Microsoft V2 - PROD Columns on file Stewardship-Telemedicine.xlsx\n",
      "Processing Guidelines & Checklist on file Stewardship-Pathology.xlsm\n",
      "Processing Global Checklist on file Stewardship-Pathology.xlsm\n",
      "Processing Team List on file Stewardship-Pathology.xlsm\n",
      "Processing Tracking & Resources on file Stewardship-Pathology.xlsm\n",
      "Processing Stewardship Tracking - DD on file Stewardship-Pathology.xlsm\n",
      "Processing Stewardship Tracking - non-MVP on file Stewardship-Pathology.xlsm\n",
      "Processing MVP Pathology Tables on file Stewardship-Pathology.xlsm\n",
      "Processing Q3 2022 Tables on file Stewardship-Pathology.xlsm\n",
      "Processing MVP Transaction Tables-Columns on file Stewardship-Pathology.xlsm\n",
      "Processing MVP - Data Dictionaries on file Stewardship-Pathology.xlsm\n",
      "Processing Guidelines & Checklist on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Team Lists on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Tracking & Resources on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Global Checklist on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Domain Meeting Recaps on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Authoritative Source Tables on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Surg-Columns(Transactional) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Sheet1 on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Surg-Columns(Dictionary) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Meds-Columns(Transactional) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing OPVists-Columns-(Transactional) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Sheet2 on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing OpVisits-Columns-(Dictionaries) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Sheet3 on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing OpVisits-Columns-(Audit) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Epic-Columns(Foundational) on file Stewardship-Epic-Clarity.xlsm\n",
      "Processing Guidelines & Checklist on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Team List on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Tracking & Resources on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Global Checklist on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Source Tables on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Reference Data on file Stewardship-RMS-Optum.xlsm\n",
      "Processing IPAdmissions on file Stewardship-RMS-Optum.xlsm\n",
      "Processing PatientRegistration on file Stewardship-RMS-Optum.xlsm\n",
      "Processing HospitalChargesClaims on file Stewardship-RMS-Optum.xlsm\n",
      "Processing HospitalPaymentsDenials on file Stewardship-RMS-Optum.xlsm\n",
      "Processing ProviderBilling on file Stewardship-RMS-Optum.xlsm\n",
      "Processing Sheet1 on file PHI-Categories.xlsx\n",
      "Processing Guidelines & Checklist on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Team List on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Global Checklist on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Tracking & Resources on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Supporting Docs on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Authoritative Source Tables on file Stewardship-MSK-Engage.xlsx\n",
      "Processing MVP on file Stewardship-MSK-Engage.xlsx\n",
      "Processing NonMVP on file Stewardship-MSK-Engage.xlsx\n",
      "Processing Guidelines & Checklist on file Stewardship-PIMS.xlsm\n",
      "Processing Team List on file Stewardship-PIMS.xlsm\n",
      "Processing Technical Checklist on file Stewardship-PIMS.xlsm\n",
      "Processing Tracking & Resources on file Stewardship-PIMS.xlsm\n",
      "Processing Source Tables on file Stewardship-PIMS.xlsm\n",
      "Processing ResearchAdmin-Columns on file Stewardship-PIMS.xlsm\n",
      "Processing Data Dictionary JM on file Stewardship-PIMS.xlsm\n",
      "Processing Guidelines & Checklist on file Stewardship-MPATH.xlsx\n",
      "Processing Team List on file Stewardship-MPATH.xlsx\n",
      "Processing Tracking & Resources on file Stewardship-MPATH.xlsx\n",
      "Processing Global Checklist on file Stewardship-MPATH.xlsx\n",
      "Processing Authoritative Source Tables on file Stewardship-MPATH.xlsx\n",
      "Processing MPATH-(Transactional) on file Stewardship-MPATH.xlsx\n",
      "Processing Data Classes on file DPR.xlsx\n",
      "Processing WKCRuleStructure on file DPR.xlsx\n",
      "Processing PHI-Date Classes on file DPR.xlsx\n",
      "Processing Delete-Classes on file DPR.xlsx\n",
      "Processing DataType Support in DV on file DPR.xlsx\n",
      "Processing User&Group Testing on file DPR.xlsx\n",
      "Processing Rule_Tests on file DPR.xlsx\n",
      "Processing data_sub_classes_wkc_phi_status on file DPR.xlsx\n",
      "Processing PHI_Classes on file DPR.xlsx\n",
      "Processing PHI_Type_Descriptions on file DPR.xlsx\n",
      "Processing Parent_PHI_Data_Classes on file DPR.xlsx\n",
      "Processing Custom_Data_Classes on file DPR.xlsx\n",
      "Processing All PHI Data Classes on file DPR.xlsx\n",
      "Processing MaskingMethodNotes on file DPR.xlsx\n",
      "Processing WKC_Data_Class_Import on file DPR.xlsx\n",
      "Processing RMS_Patient_Table_PHI_Classes on file DPR.xlsx\n",
      "Processing Patient_IDs_Identifiers on file DPR.xlsx\n",
      "Processing Sheet1 on file DPR.xlsx\n",
      "Processing Sheet2 on file DPR.xlsx\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./Stewardship/*')\n",
    "udf = pd.DataFrame(columns=['System','Table Name','Column','Vendor Definition','Enterprise Concept','Working Definition','Authoritative?','UDF Ingest?','PHI Indicator','Data Class','Squad','Status'])\n",
    "for file in files:\n",
    "  df = pd.ExcelFile(file)\n",
    "  sheets = df.sheet_names\n",
    "  # print(file.replace('./Stewardship/',''), df.sheet_names)\n",
    "  for sheet in sheets:\n",
    "    if 'CCDE' in file:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Processing {sheet} on file {file.replace('./Stewardship/','')}\")\n",
    "      f = df.parse(sheet)\n",
    "      if 'Working Definition' in f.columns:\n",
    "        if 'Tag' in f.columns:\n",
    "          pass\n",
    "        else:\n",
    "          f['Tag'] = np.NaN\n",
    "        if 'Telemedicine' in file:\n",
    "          f = f.drop(['Source Table (Telemed DB)','Source Column'], axis=1)\n",
    "          f.columns = ['ID', 'Squad', 'Table Name', 'Column', '% null', '% fill', 'Vendor Definition', 'Manually tagged in WKC?', 'Enterprise Concept', 'Working Definition', 'Related Concept', 'Allowable Values \\n(if applicable) ', 'Authoritative?', 'UDF Ingest?', 'UDF Environments', 'PHI Indicator', 'Data Class', 'Tag', 'PHI DType', 'Masking Required/ Supported DType? (Y/N)', 'Notes from Stewardship', 'Notes from Data Stewards', 'Scope Review Notes', 'Questions / Follow-up with Stewards', 'Status', 'Data Class Status', 'Date PHI Class  Signed off (Stewardship Team)', 'Date Class Tagged in WKC', 'Catalog Masking Observed (4.05)', 'DV/Dbeaver Masking Observed (4.05) ', 'Testing account', 'Notes', 'Address  Line 3']\n",
    "        f['System'] = file.replace('./Stewardship/','').replace('.xlsm','').replace('Stewardship-','').replace('.xlsx','')\n",
    "        newdf = f[['System','Table Name','Column','Vendor Definition','Enterprise Concept','Working Definition','Authoritative?','UDF Ingest?','PHI Indicator','Data Class','Tag','Squad', 'Notes from Stewardship', 'Notes from Data Stewards', 'Scope Review Notes', 'Questions / Follow-up with Stewards','Status']]\n",
    "        udf['Table Name'] = udf['Table Name'].str.lower().replace(' ','')\n",
    "        udf['Column'] = udf['Column'].str.strip()\n",
    "        udf['Column'] = udf['Column'].str.lower()\n",
    "        udf = pd.concat([udf,newdf], axis=0, ignore_index=True)\n",
    "        # print(udf.shape[0])\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "for col in udf.columns:\n",
    "  udf[col] = udf[col].str.strip()\n",
    "udf = udf.fillna('')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 6)\n",
      "(84, 6)\n"
     ]
    }
   ],
   "source": [
    "iic = pd.DataFrame(udf[['System','Table Name','Column','Enterprise Concept']][(udf['Enterprise Concept'].str.contains('Identifier')) | (udf['Enterprise Concept'].str.contains('Internal Identifier'))]).reset_index(drop=True)#.drop('index',axis=1)\n",
    "# iic['Enterprise Concept'] = iic['Enterprise Concept'].str.replace(r' \\n',' ', regex=True).replace(r'\\n',' ', regex=True)\n",
    "iic[['root','suffix','a']] = iic['Enterprise Concept'].str.split('Internal', expand=True)\n",
    "iic[['root','suffix1','a']] = iic['root'].str.split('Identifier',expand= True)\n",
    "iic['suffix'] = iic['suffix'].str.replace('Identifier','Internal Identifier').fillna('Identifier')\n",
    "iic = iic.drop(['suffix1','a'],axis=1).sort_values(['Enterprise Concept','suffix']).drop_duplicates()\n",
    "iic1 = iic[iic.duplicated('root',keep=False)].reset_index(drop=True)\n",
    "iic1 = iic1.drop_duplicates(subset=['root','suffix'])\n",
    "iic1['suffix'] = iic1['suffix'].str.strip()\n",
    "print(iic1.shape)\n",
    "\n",
    "iic2 = iic.drop_duplicates(subset=['System','Table Name','Column','root'],keep='last')\n",
    "iic2 = iic2[iic2['suffix'].str.contains('Internal Identifier')]\n",
    "print(iic2[iic2['System'] == 'Pathology'].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we import the IBM export from the IGC site here: https://zen-cpd-zen.msk-udf-cpd-prod-f5abc063be4517f30ecdedb296bcfb96-0000.us-east.containers.appdomain.cloud/ibm/iis/igc/\n",
    "In this platform, we will use the Queries module and the following queries to extract the needed files:\n",
    "- Database Schema, Table, Column, Definitions and Concepts -> which we will save under the name 'IBM-catalog-extract'\n",
    "- Database Columns and their Data Classes -> which we will save under the name 'IBM-catalog-dataclass'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "cat = pd.read_csv('IBM-catalog-extract.csv', encoding='unicode_escape')#.drop(['Asset Name'], axis=1)\n",
    "dc = pd.read_csv('IBM-catalog-dataclass.csv').drop_duplicates(subset=['Schema','Table Name','Column'],keep='last').fillna('')\n",
    "\n",
    "cp4d = cat.merge(dc, on=['Schema','Table Name','Column'],how='left').fillna('').sort_values(['Schema','Table Name','Column'], ignore_index=True)\n",
    "\n",
    "cp4d['Table Name'] = cp4d['Table Name'].str.replace('_V','')\n",
    "cp4d['Table Name'] = cp4d['Table Name'].str.lower()\n",
    "cp4d['Column'] = cp4d['Column'].str.lower()\n",
    "\n",
    "for idx, row in cp4d.iterrows():\n",
    "    if row['Schema'] == 'UDF_RMS_RAW_DV':\n",
    "        row['Table Name'] = row['Table Name'].split('__')[0].replace('_','')\n",
    "for col in cp4d.columns:\n",
    "  cp4d[col] = cp4d[col].str.strip()\n",
    "\n",
    "# cp4d = cp4d.drop(['asset','rule_name','rule_description','rule_action'],axis=1).fillna('')\n",
    "# cp4d = cp4d[['Schema', 'Table','column_name', 'column_terms', 'column_tags', 'data_class']].fillna('')\n",
    "cp4d.columns = ['Schema', 'Table Name','Column', 'WKC Concept Name', 'WKC Definition', 'Data Class']\n",
    "cp4d = cp4d.fillna('')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by checking the concept names and whether they match between our Templates and WKC currently."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "conc = udf.merge(wkc, left_on=['Enterprise Concept'], right_on=['Name'], indicator='Template or WKC')\n",
    "conc = conc[['System','Enterprise Concept','Working Definition','Description','Template or WKC']]\n",
    "conc.columns = ['System','Enterprise Concept','Working Definition','WKC Definition','Template or WKC']\n",
    "conc['Def_Match'] = np.where(conc['Working Definition'] == conc['WKC Definition'],'Y','N')\n",
    "conc = conc.drop_duplicates(subset=['Enterprise Concept','Working Definition'], keep='first')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will create a combined IBM master file that contains important metadata, such as Authoritative Sources, Data Class, Council Approval date, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "ibm = wkc.merge(cp4d, left_on='Name', right_on='WKC Concept Name',how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we will merge our Stewardship spreadsheet information to the combined IBM master file. This will be our guide for our cleanup.\n",
    "We do this by merging the two sets based on the Table Name and column values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df1 = udf.merge(ibm, on=['Table Name','Column'], suffixes=['_Template','_IBM'], how='left', indicator='Template or IBM')\n",
    "df1 = df1.replace(np.NaN, '')\n",
    "# print(df1.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we will extract the concepts where the Concept names do not match between UDF stewardship templates and CP4D extract"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "cnm = df1[['System', 'Table Name', 'Column','Enterprise Concept', 'WKC Concept Name','Template or IBM']][(df1['Enterprise Concept'] != df1['WKC Concept Name']) & (df1['Status'].isin(['Complete','Signed Off Stewardship']))].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's find those records where the Working Definition either does not match to that in IBM, or is blank."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "dnm = df1[['System', 'Table Name', 'Column','Enterprise Concept','Name', 'Working Definition','Description','Template or IBM']][((df1['Working Definition'] != '') & (df1['Working Definition'] != df1['Description']) & (df1['Name'] != '')) & (df1['Status'].isin(['Complete','Signed Off Stewardship','Sign off Complete']))].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we will extract the concepts indicated as PHI in our stewardship templates that do ***not*** have a matching **Data Class** in CP4D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "phidc = df1[['System', 'Table Name', 'Column','Data Class_Template', 'Data Class_IBM','Tag','Template or IBM']][(df1['PHI Indicator'].isin(['y','Y','Yes','YES','PHI'])) & (df1['Data Class_Template'] != df1['Data Class_IBM'])].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another check here is to identify columns that could potentially be PHI that were not so during stewardship"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHI Check for Account with parameters acc.*num ------------------------------------------------------------\n",
      "PHI Check for Address with parameters addr.*lin,addr.* ------------------------------------------------------------\n",
      "PHI Check for Address with parameters postal.*,zip.* ------------------------------------------------------------\n",
      "PHI Check for Address with parameters province ------------------------------------------------------------\n",
      "PHI Check for Address with parameters city,town ------------------------------------------------------------\n",
      "PHI Check for Address with parameters latitude,longitude,geographic,geo.*coor ------------------------------------------------------------\n",
      "PHI Check for Address with parameters eircode ------------------------------------------------------------\n",
      "PHI Check for Address with parameters post.*code ------------------------------------------------------------\n",
      "PHI Check for Address with parameters county ------------------------------------------------------------\n",
      "PHI Check for Address with parameters state ------------------------------------------------------------\n",
      "PHI Check for Address with parameters street ------------------------------------------------------------\n",
      "PHI Check for Biometric with parameters biometric,finger,voice ------------------------------------------------------------\n",
      "PHI Check for Credit Card with parameters credit.*card.* ------------------------------------------------------------\n",
      "PHI Check for Date of Birth with parameters birth.*,dob ------------------------------------------------------------\n",
      "PHI Check for Date/Time with parameters date,time,stamp,_dt,.*dtm,utc,dt_ ------------------------------------------------------------\n",
      "PHI Check for Device with parameters host.*nam ------------------------------------------------------------\n",
      "PHI Check for Device with parameters mob.*phon.*,phone,imei,mob.*id.*,tel_.*,.*_tel ------------------------------------------------------------\n",
      "PHI Check for Device with parameters internet protocol,ip.*address ------------------------------------------------------------\n",
      "PHI Check for Document with parameters doc.*id ------------------------------------------------------------\n",
      "PHI Check for Driver License with parameters driver.*lic ------------------------------------------------------------\n",
      "PHI Check for Email with parameters email ------------------------------------------------------------\n",
      "PHI Check for Free Text with parameters free text,ft,comments,sum.*line,memo,text,subject,body,value,data,reason,instruc,indica ------------------------------------------------------------\n",
      "PHI Check for Health Plan with parameters insurance.*,policy.*,certificate.*number,plan ------------------------------------------------------------\n",
      "PHI Check for Image with parameters face,photo,image ------------------------------------------------------------\n",
      "PHI Check for Identifier with parameters particip.*id ------------------------------------------------------------\n",
      "PHI Check for Location with parameters loc_.*id,loca.*id,location,temp.*loc,room,bed,or_ ------------------------------------------------------------\n",
      "PHI Check for Name with parameters first.*name,last.*name,middle.*name,pers.*name,disp.*name,guar.*nm,guar.*name,pat.*nm,_nm ------------------------------------------------------------\n",
      "PHI Check for Order/Task/Administration with parameters order.*id,order.*guid,order.*num,task.*occ.*id,presc.*id ------------------------------------------------------------\n",
      "PHI Check for Patient with parameters mrn,med.*rec.*num,pat.*id,pat.*num,pt.*id,pt.*num ------------------------------------------------------------\n",
      "PHI Check for Patient with parameters adm_,admit ------------------------------------------------------------\n",
      "PHI Check for Patient with parameters cust.*num ------------------------------------------------------------\n",
      "PHI Check for Patient with parameters person,individual ------------------------------------------------------------\n",
      "PHI Check for Social Number with parameters social.*num,sin ------------------------------------------------------------\n",
      "PHI Check for Social Number with parameters .*ssn.*,soc.*sec.*num ------------------------------------------------------------\n",
      "PHI Check for Task with parameters task.*id ------------------------------------------------------------\n",
      "PHI Check for Test/Sample with parameters spec.*id,spec.*num,acces.*id,acces.*num,sample ------------------------------------------------------------\n",
      "PHI Check for URL with parameters url,link ------------------------------------------------------------\n",
      "PHI Check for Vehicle with parameters reg.*num,vehicle.*num,car.*num ------------------------------------------------------------\n",
      "PHI Check for Vehicle with parameters vin,veh.*num,serial.*num,chassis.*num ------------------------------------------------------------\n",
      "PHI Check for Visit/Encounter with parameters visit.*id,enc.*id,chart.*id,dmp.*id,epic.*num,visit.*num,epic.*id ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "phic = pd.read_excel('./Stewardship/PHI-Categories.xlsx')\n",
    "phir = pd.DataFrame()\n",
    "for idx, row in phic.iterrows():\n",
    "    cl = row['Regex'].split(',')\n",
    "    l = '|'.join(cl)\n",
    "    cm = row['Regex2'].split(',')\n",
    "    m = '|'.join(cm)\n",
    "    print(f\"PHI Check for {row['Potential PHI']} with parameters {row['Regex']} {'---'*20}\")\n",
    "    phint = df1[(df1['Column'].str.contains(l, regex=True, case=False)) | (df1['Enterprise Concept'].str.contains(m, regex=True, case=False))].reset_index(drop=True)\n",
    "\n",
    "    # phint = df1[(df1['Enterprise Concept'].str.contains(m, regex=True, case=False))].reset_index(drop=True)\n",
    "    phint['Potential PHI - Column'] = np.where(phint['Column'].str.contains(l,regex=True, case=False), row['Potential PHI'],'')\n",
    "    phint['Potential PHI - Concept'] = np.where(phint['Enterprise Concept'].str.contains(m,regex=True, case=False), row['Potential PHI'],'')\n",
    "    # print(phint1)\n",
    "    phint = phint[['System', 'Table Name', 'Column','Enterprise Concept', 'Working Definition', 'Squad', 'Potential PHI - Column','Potential PHI - Concept','Data Class_Template', 'WKC Concept Name', 'Data Class_IBM','Template or IBM']]\n",
    "    phir = pd.concat([phir,phint], axis=0, ignore_index=True).drop_duplicates(subset=['Table Name','Column'],keep='last')\n",
    "phir = phir[(phir['Potential PHI - Column'] != '' )| (phir['Potential PHI - Concept'] != '')]\n",
    "# print(phir[['System','Column','Enterprise Concept','Working Definition','Potential PHI - Column','Potential PHI - Concept']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, Let\\'s check that the Data Classes currently attached to the terms in the Stewardship templates are the ones up-to-date."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dpr = pd.read_excel('./Stewardship/DPR.xlsx').fillna('')\n",
    "dcw = pd.read_excel('WKC-Data-classes.xlsx').fillna('')\n",
    "dcr = dcw.merge(dpr, on='Data Class', suffixes=['_IBM','_MSK'], indicator=True, how='left')\n",
    "dcr = dcr[['Custom/OOB',\t'PHI/PII',\t'PHI Category',\t'Parent Data Class',\t'Data Class',\t'Data Type',\t'Max Length',\t'Min Length',\t'Description_IBM', 'RegEx']]\n",
    "\n",
    "### DUPLICATING COLUMN TO HAVE SEPARATE COLUMNS ON JOIN ###\n",
    "dpr['Data Class_DPR'] = dpr['Data Class']\n",
    "dpr = dpr.drop('Data Class', axis=1)\n",
    "\n",
    "dprnm = udf.merge(dpr, left_on='Data Class', right_on='Data Class_DPR', how='left', indicator='Template or IBM')\n",
    "dprnm['PHI/PII'] = dprnm['PHI/PII'].fillna('')\n",
    "dprnm = dprnm[['System', 'Table Name','Column','Enterprise Concept','Working Definition','Data Class','Data Class_DPR','PHI/PII','PHI Indicator']][(dprnm['Data Class'] != '') & (dprnm['Data Class'] != dprnm['Data Class_DPR'])].replace(np.NaN,'').reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we will create an Excel file with all our findings, and save each of the data sets we've created into their own tab in the file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "file = 'DC-Cleanup1.xlsx'\n",
    "writer = pd.ExcelWriter(file, engine='xlsxwriter')\n",
    "udf.to_excel(writer, index=False, sheet_name='Master')\n",
    "conc.to_excel(writer,index=False, sheet_name='Concept-in-WKC')\n",
    "cnm.to_excel(writer, index=False, sheet_name='ConceptName-Mismatch')\n",
    "iic1.to_excel(writer, index=False, sheet_name='InternalIdent-vs-Ident')\n",
    "iic2.to_excel(writer, index=False, sheet_name='InternalIdent-Change')\n",
    "dnm.to_excel(writer, index=False, sheet_name='Definition-Mismatch')\n",
    "dcr.to_excel(writer, index=False, sheet_name='PHI-DataClass')\n",
    "phidc.to_excel(writer, index=False, sheet_name='PHI-DataClass-Mismatch')\n",
    "phir.to_excel(writer, index=False, sheet_name='PHI-Review')\n",
    "dprnm.to_excel(writer, index=False, sheet_name='DataClass-Update')\n",
    "writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
